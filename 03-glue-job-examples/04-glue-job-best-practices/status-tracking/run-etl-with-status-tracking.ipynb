{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49ddb27",
   "metadata": {},
   "source": [
    "# Run ETL with Status Tracking\n",
    "\n",
    "[CN]\n",
    "\n",
    "**挑战**\n",
    "\n",
    "我们希望对每一个文件的 ETL 是否成功进行追踪.\n",
    "\n",
    "**解决方案**\n",
    "\n",
    "AWS Glue ETL 的编程模型包含三个重要概念, Input -> Transformation -> Output. Status Tracking 的最小单位就是一个个的 Input. 如果 Source 是 S3, 那么 Input 则是一个个的文件. \n",
    "\n",
    "所以我们只要把处理每一个单个文件的 ETL 逻辑封装成一个函数. 调用这个函数前创建一条 Dynamodb 记录. 如果抛出了异常则更新 status attribute. 这样我们只要查询 Dynamodb 数据库就知道哪些成功了哪些不成功.\n",
    "\n",
    "我们考虑一个非常简单的 JSON to Parquet 的 ETL 逻辑. 数据用的是人造数据, 模拟银行的 Transaction 流水. 详情请参考 ``dataset.ipynb`` 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f86f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>5</td><td>application_1646085135716_0032</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-8-11.us-east-2.compute.internal:20888/proxy/application_1646085135716_0032/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-25-26.us-east-2.compute.internal:8042/node/containerlogs/container_1646085135716_0032_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70312b0f27a6483e96522b0f1cf5c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abc1577f5fe497ea3b039d3a6f69cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pyspark.sql.functions as sql_funcs\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "\n",
    "# Create SparkContext\n",
    "sparkContext = SparkContext.getOrCreate()\n",
    "\n",
    "# Create Glue Context\n",
    "glueContext = GlueContext(sparkContext)\n",
    "\n",
    "# Get spark session\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Resolve job parameters\n",
    "# Uncomment this in Glue ETL job\n",
    "# args = getResolvedOptions(sys.argv, [\"JOB_NAME\"\n",
    "# job = Job(glueContext)\n",
    "# job.init(args['JOB_NAME'], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc995831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f775d1e45a14e7d981d8f14ab919aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)"
     ]
    }
   ],
   "source": [
    "from s3pathlib import S3Path\n",
    "\n",
    "class Config:\n",
    "    region = \"us-east-2\"\n",
    "    source_bucket = \"aws-data-lab-sanhe-for-everything-us-east-2\"\n",
    "    source_prefix = \"poc/learn-big-data-on-aws/glue-job-examples/04-glue-job-best-practice/status-tracking/bank_transaction/source/\"\n",
    "    target_bucket = \"aws-data-lab-sanhe-for-everything-us-east-2\"\n",
    "    target_prefix = \"poc/learn-big-data-on-aws/glue-job-examples/04-glue-job-best-practice/status-tracking/bank_transaction/target/\"\n",
    "    \n",
    "    n_file = 100 # 一共多少个文件\n",
    "    n_rows_per_file = 1000 # 每个文件由多少行\n",
    "    n_acc = 20000 # 模拟多少个银行账户互相转账\n",
    "    failed_rate = 10 # 按照百分之几的比例创建 \"坏\" 文件, 5 就是 5%\n",
    "    \n",
    "    @property\n",
    "    def s3path_source(self) -> S3Path:\n",
    "        return S3Path(self.source_bucket, self.source_prefix)\n",
    "\n",
    "    @property\n",
    "    def s3path_target(self) -> S3Path:\n",
    "        return S3Path(self.target_bucket, self.target_prefix)\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a300e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebcc5586230412fba09768b956a0cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a1bcc",
   "metadata": {},
   "source": [
    "## Define Dynamodb Schema\n",
    "\n",
    "Dynamodb 是一个高性能, 无需管理基础设置, 自动 Scale 的 Key Value Store 数据库. 特别适合用来记录每个 S3 File 的状态. 我们可以用 0 标记 TODO, 1 标记处理失败, 2 标记处理成功.\n",
    "\n",
    "这里需要注意一点. 下面的代码在 Laptop 本地的开发环境里是可以用来创建 Dynamodb Table 的. 但是在 Glue Dev Endpoint 中, 由于用的版本是 Glue 1.0, 也就是 Python3.6, 其中的 botocore 版本过低, 导致不支持 pynamodb 中的 create_table API 中的一些参数. **所以建议现在本地用下面的代码预先创建好 Dynamodb Table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d88398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e070a7d9aab4168858bc5472fdb85e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pynamodb\n",
    "from pynamodb.models import Model, GlobalSecondaryIndex\n",
    "from pynamodb.connection import Connection\n",
    "from pynamodb.attributes import UnicodeAttribute, NumberAttribute\n",
    "from pynamodb.indexes import KeysOnlyProjection\n",
    "\n",
    "# create boto3 dynamodb client connection with default AWS profile\n",
    "connection = Connection()\n",
    "\n",
    "class Status:\n",
    "    todo = 0\n",
    "    failed = 1\n",
    "    success = 2\n",
    "\n",
    "\n",
    "class StatusIndex(GlobalSecondaryIndex):\n",
    "    class Meta:\n",
    "        index = \"status-index\"\n",
    "        projection = KeysOnlyProjection\n",
    "\n",
    "    status = NumberAttribute(hash_key=True)\n",
    "    s3uri = UnicodeAttribute()\n",
    "\n",
    "\n",
    "# Create bank account data model\n",
    "class Tracker(Model):\n",
    "    class Meta:\n",
    "        \"\"\"\n",
    "        declare metadata about the table\n",
    "        \"\"\"\n",
    "        table_name = \"learn_big_data_on_aws_glue_tracker\"\n",
    "        region = \"us-east-2\"\n",
    "\n",
    "        # billing mode\n",
    "        # doc: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n",
    "        # pay as you go mode\n",
    "        billing_mode = pynamodb.models.PAY_PER_REQUEST_BILLING_MODE\n",
    "\n",
    "        # provisioned mode\n",
    "        # write_capacity_units = 10\n",
    "        # read_capacity_units = 10\n",
    "\n",
    "    # define attributes\n",
    "    s3uri = UnicodeAttribute(hash_key=True)\n",
    "    status = NumberAttribute(default=Status.todo)\n",
    "\n",
    "    status_index = StatusIndex()\n",
    "\n",
    "# Create dynamodb table if not exists, if already exists, this code won't do anything\n",
    "# Don't use this with PySpark Magic kernel on Glue 1.0 Dev Endpoint, the botocore version is too old,\n",
    "# and don't have ``BillingMode`` argument\n",
    "Tracker.create_table(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defbf2",
   "metadata": {},
   "source": [
    "Glue / PySpark 的 ``from_options`` API 支持读取一个 S3 Folder. 然后 Spark 内部就会让各个 Node 同时开始并行读取, 然后将所有数据抽象成一个大型的 DataFrame. 这么做有个缺点, 数据都进入到了大型 DataFrame, 但你无从得知哪一行数据属于哪一个文件.\n",
    "\n",
    "如果你用的是 ``from_catalog`` API 则可以用 ``input_file_name`` API 和 ``withColumn`` API 来获得原始文件名, 并将其添加到一列.\n",
    "\n",
    "但无论那种情况, 你都无法做到对每一个文件进行 ETL Transformation, 然后一旦出错, 就在 Dynamodb 中将其标记为 failed.\n",
    "\n",
    "所以为了获得更好的 tracking 能力, 我们将对单个文件进行处理的逻辑封装成了函数, 然后对所有的 s3 files 进行遍历, 然后用 try exception 来包围这个函数的调用. 一旦有异常, 我们则将其标记为失败. 这样做的代价是无法利用并行处理, 处理速度会变慢, 好处是我们对每个文件都有很详细的追踪能力. 这种做法我们最好确保每个文件都比较大, 这样性能损失不会很多. 如果每个文件都很小, 你不如直接用 Lambda 做 transformation 会更好处理.\n",
    "\n",
    "```scala\n",
    "var df = glueContext.getCatalogSource(\n",
    "  database = database,\n",
    "  tableName = table,\n",
    "  transformationContext = s\"source-$database.$table\"\n",
    ").getDynamicFrame()\n",
    " .toDF()\n",
    " .withColumn(\"input_file_name\", input_file_name())\n",
    "\n",
    "glueContext.getSinkWithFormat(\n",
    "  connectionType = \"s3\",\n",
    "  options = JsonOptions(Map(\n",
    "    \"path\" -> args(\"DST_S3_PATH\")\n",
    "  )),\n",
    "  transformationContext = \"\",\n",
    "  format = \"parquet\"\n",
    ").writeDynamicFrame(DynamicFrame(df, glueContext))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c0c19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b27a561a9a44f192ff931a39f83649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件已被处理完毕!, 耗时 142.259167 秒..."
     ]
    }
   ],
   "source": [
    "def run_glue_etl_logic(s3path: S3Path):\n",
    "    try:\n",
    "        tracker = Tracker.get(s3path.uri)\n",
    "    except Tracker.DoesNotExist:\n",
    "        tracker = Tracker(s3uri=s3path.uri)\n",
    "        tracker.save()\n",
    "    \n",
    "    if tracker.status != Status.todo:\n",
    "        return\n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Step 1. Read the Data\n",
    "    #-----------------------------------------------------------------------------\n",
    "    gdf = glueContext.create_dynamic_frame.from_options(\n",
    "        connection_type=\"s3\", \n",
    "        connection_options=dict(\n",
    "            paths=[s3path.uri,],\n",
    "            recurse=True,\n",
    "        ),\n",
    "        format=\"json\",\n",
    "        format_options=dict(multiLine=True),\n",
    "        transformation_ctx=\"datasource\",\n",
    "    )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Step 2. Transform the data\n",
    "    #-----------------------------------------------------------------------------\n",
    "    columns = gdf.toDF().columns\n",
    "    if set(columns) != set([\"trans_id\", \"from_acc\", \"to_acc\", \"balance\", \"created_time\"]):\n",
    "        raise ValueError\n",
    "        \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Step 3. Write the data\n",
    "    #-----------------------------------------------------------------------------\n",
    "    data_sink_parquet = glueContext.write_dynamic_frame.from_options(\n",
    "        frame=gdf,\n",
    "        connection_type=\"s3\",\n",
    "        connection_options={\n",
    "            \"path\": config.s3path_target.uri,\n",
    "        },\n",
    "        format=\"parquet\",\n",
    "        transformation_ctx=\"DataSink_parquet\"\n",
    "    )\n",
    "        \n",
    "st = datetime.now()\n",
    "for s3path in config.s3path_source.iter_objects():\n",
    "    tracker = Tracker(s3uri=s3path.uri)\n",
    "    try:\n",
    "        run_glue_etl_logic(s3path)\n",
    "        tracker.status = 2\n",
    "    except Exception as e:\n",
    "        tracker.status = 1\n",
    "    tracker.save()\n",
    "et = datetime.now()\n",
    "elapse = (et - st).total_seconds()\n",
    "print(f\"所有文件已被处理完毕!, 耗时 {elapse} 秒...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf42ee8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afbf5c3ae2d45c8953ab0a8d2ddaeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf_total = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\", \n",
    "    connection_options=dict(\n",
    "        paths=[config.s3path_target.uri,],\n",
    "        recurse=True,\n",
    "    ),\n",
    "    format=\"parquet\",\n",
    "    transformation_ctx=\"gdf_total\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56f380e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edbf64ad22f44a9ac80ec05e6567e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 目录下所有文件中实际有 86000 条记录"
     ]
    }
   ],
   "source": [
    "total_rows_in_target = gdf_total.toDF().count()\n",
    "print(f\"target 目录下所有文件中实际有 {total_rows_in_target} 条记录\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee8df28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d780f59fdbf4defa76402072c4a8ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 目录下所有文件中应该有 86000 条记录"
     ]
    }
   ],
   "source": [
    "# 查看成功的文件总数有多少, \n",
    "success_file_count = StatusIndex.count(hash_key=Status.success)\n",
    "expected_target_rows = success_file_count * config.n_rows_per_file\n",
    "print(f\"target 目录下所有文件中应该有 {expected_target_rows} 条记录\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
