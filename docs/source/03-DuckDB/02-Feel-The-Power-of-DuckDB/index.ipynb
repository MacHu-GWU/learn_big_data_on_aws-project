{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c791777-4503-45f4-9a2c-9383258f73dc",
   "metadata": {},
   "source": [
    "# Feel The Power of DuckDB\n",
    "\n",
    "Let's feel the power of DuckDB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be08fba-78a1-47e7-8dfa-4b214b8da085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "import textwrap\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import mpire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from faker import Faker\n",
    "from s3pathlib import S3Path, context\n",
    "from boto_session_manager import BotoSesManager\n",
    "from fixa.timer import DateTimeTimer\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e4c0d-d4ad-4aaf-924f-6cbf3a847c7a",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a54fae-8255-46f2-921a-1dfe9295fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preview s3dir_root: https://console.aws.amazon.com/s3/buckets/807388292768-us-east-1-data?prefix=projects/duckdb/db/\n"
     ]
    }
   ],
   "source": [
    "# define the boto3 session for this POC\n",
    "bsm = BotoSesManager(profile_name=\"awshsh_app_dev_us_east_1\")\n",
    "\n",
    "# set the boto3 session as the default for s3pathlib\n",
    "context.attach_boto_session(bsm.boto_ses)\n",
    "\n",
    "# this is the bucket where we upload test data\n",
    "s3bucket = S3Path(f\"s3://{bsm.aws_account_id}-{bsm.aws_region}-data/\")\n",
    "\n",
    "# this is the S3 location for the \"database\"\n",
    "s3dir_root = s3bucket.joinpath(\"projects\", \"duckdb\", \"db\").to_dir()\n",
    "\n",
    "# this is the S3 location for the \"table\"\n",
    "s3dir_t_poc = s3dir_root.joinpath(\"poc\").to_dir()\n",
    "\n",
    "# we need the faker library to generate dummy text data\n",
    "fake = Faker()\n",
    "\n",
    "print(f\"preview s3dir_root: {s3dir_root.console_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5482a0b-33c9-40ad-96f7-d067f3dcfb30",
   "metadata": {},
   "source": [
    "## Create Test Data\n",
    "\n",
    "The test data has four columns:\n",
    "\n",
    "- ``id``: integer, from 1, 2, 3, ..., is the globally unique id for each record\n",
    "- ``number``: integer, is a random value from 0 ~ 1,000,000, we can use this column for range query\n",
    "- ``time_str``: string, is a timestamp in ``%Y-%m-%d %H:%M:%S.%f`` format, the vlaue is from ``2022-01-01 00:00:00`` to ``2023-01-01 00:00:00`` we can use this column for range query\n",
    "- ``text``: string, is a random text, we can use this column for full-text-search query\n",
    "\n",
    "the ``create_data`` function can create a ``data.parquet`` file at the ``s3://my-bucket/prefix/date=${date}/data.parquet`` location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9cc2f6-4068-4e3d-850e-e39e27b0d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_data():\n",
    "    s3dir_root.delete()\n",
    "\n",
    "\n",
    "def create_data(ith: int, date: str, dry_run: bool = False):\n",
    "    print(f\"working on {date} ...\")\n",
    "    n_row = 1000000\n",
    "    df = pd.DataFrame()\n",
    "    df[\"id\"] = range((ith-1) * n_row + 1, ith * n_row + 1)\n",
    "    df[\"number\"] = np.random.randint(0, 1000000, n_row)\n",
    "    df[\"time_str\"] = pd.to_datetime(\n",
    "        np.random.choice(\n",
    "            pd.date_range(\"2022-01-01\", \"2023-01-01\", periods=n_row),\n",
    "            n_row,\n",
    "            replace=False,\n",
    "        )\n",
    "    )\n",
    "    df[\"text\"] = [fake.sentence() for _ in range(n_row)]\n",
    "    df = pl.from_pandas(df)\n",
    "    s3path = s3dir_t_poc.joinpath(f\"date={date}\", \"data.parquet\")\n",
    "    if dry_run is False:\n",
    "        with s3path.open(\"wb\") as f:\n",
    "            df.write_parquet(f, row_group_size=250000)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdea38-cf68-4172-8e93-56fd3b004f42",
   "metadata": {},
   "source": [
    "Below is a sample data created by the ``create_data`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5947bb7a-bf5a-4e45-aad8-a904d9cc8313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 2021-01-01\n",
      "shape: (1_000_000, 4)\n",
      "┌─────────┬────────┬───────────────────────────────┬───────────────────────────────────────────────┐\n",
      "│ id      ┆ number ┆ time_str                      ┆ text                                          │\n",
      "│ ---     ┆ ---    ┆ ---                           ┆ ---                                           │\n",
      "│ i64     ┆ i64    ┆ datetime[ns]                  ┆ str                                           │\n",
      "╞═════════╪════════╪═══════════════════════════════╪═══════════════════════════════════════════════╡\n",
      "│ 1       ┆ 679064 ┆ 2022-08-08 09:02:44.106164108 ┆ Particularly second accept record             │\n",
      "│         ┆        ┆                               ┆ professional actually scene.                  │\n",
      "│ 2       ┆ 615755 ┆ 2022-09-03 13:20:54.432054432 ┆ Something democratic benefit training common. │\n",
      "│ 3       ┆ 344864 ┆ 2022-05-07 07:29:50.609390610 ┆ Value yes soldier inside worker successful    │\n",
      "│         ┆        ┆                               ┆ term number.                                  │\n",
      "│ 4       ┆ 42400  ┆ 2022-10-18 12:31:44.301104300 ┆ Even vote issue trial door.                   │\n",
      "│ …       ┆ …      ┆ …                             ┆ …                                             │\n",
      "│ 999997  ┆ 583034 ┆ 2022-01-12 09:25:34.152334152 ┆ Say fill move direction who each.             │\n",
      "│ 999998  ┆ 724490 ┆ 2022-02-02 19:26:13.874773875 ┆ Candidate second character nor.               │\n",
      "│ 999999  ┆ 480276 ┆ 2022-02-10 18:15:54.785754786 ┆ Present serve then commercial accept weight   │\n",
      "│         ┆        ┆                               ┆ fly.                                          │\n",
      "│ 1000000 ┆ 892577 ┆ 2022-12-02 12:37:13.917433916 ┆ Hold any security pull.                       │\n",
      "└─────────┴────────┴───────────────────────────────┴───────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# create on day's data, just for preview\n",
    "df = create_data(1, date=\"2021-01-01\", dry_run=True)\n",
    "with pl.Config(fmt_str_lengths=120):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930767f-bcdc-4117-ad0d-fab5796cab2a",
   "metadata": {},
   "source": [
    "``create_all_data`` is a wraper of the ``create_data`` function. It leverage multiple CPU core to create 100 files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76063896-e0c8-4804-ba51-9b82774d5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_data():\n",
    "    print(f\"preview at: {s3dir_t_poc.console_url}\")\n",
    "    s3dir_t_poc.delete()\n",
    "\n",
    "    kwargs_list = list()\n",
    "    n_date = 100\n",
    "    start_date = date(2021, 1, 1)\n",
    "    for i in range(n_date):\n",
    "        new_date = start_date + timedelta(days=i)\n",
    "        kwargs = {\"ith\": i+1, \"date\": new_date}\n",
    "        kwargs_list.append(kwargs)\n",
    "\n",
    "    with mpire.WorkerPool(n_jobs=12) as pool:\n",
    "        pool.map(create_data, kwargs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6efd0034-126b-4616-8f1e-c6f4d59fedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all test data\n",
    "# create_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eadba2-67e8-47b5-8b8b-118ba2794f15",
   "metadata": {},
   "source": [
    "Below is the spec of the test dataset:\n",
    "\n",
    "- We have 100 days' data from 2021-01-01 to 2021-04-10, data is partitioned by date\n",
    "- Each day has 1M records, total size for each day is around 25MB\n",
    "- total size for 100 day's data is 2.5GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64f8b0e5-8299-4b9a-9122-5a46ffacf066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_file = 100\n",
      "total_size = 2.54 GB\n"
     ]
    }
   ],
   "source": [
    "n_file, total_size = s3dir_t_poc.calculate_total_size(for_human=True)\n",
    "print(f\"n_file = {n_file}\")\n",
    "print(f\"total_size = {total_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b66373-7daa-4704-a01c-23b77821dcd8",
   "metadata": {},
   "source": [
    "## Declare Common SQL Snippet\n",
    "\n",
    "Here we declare some common SQL command that will be used frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64bdb0f4-3a75-46c1-bfaa-e5b3a376bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common SQL snippet\n",
    "# enable the httpfs (HTTP file system plugin https://duckdb.org/docs/extensions/httpfs), so we can read data from AWS S3\n",
    "sql_httpfs = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "INSTALL httpfs;\n",
    "LOAD httpfs;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# enable the fts (full text search plugin https://duckdb.org/docs/extensions/full_text_search), so we can build a full text search index\n",
    "sql_fts = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "INSTALL fts;\n",
    "LOAD fts;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# set AWS credential\n",
    "sql_credential = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "SET s3_region='us-east-1';\n",
    "SET s3_access_key_id='{bsm.boto_ses.get_credentials().access_key}';\n",
    "SET s3_secret_access_key='{bsm.boto_ses.get_credentials().secret_key}';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# this is the 100 days' data source that leveraging the \"date\" partition\n",
    "sql_from_table_all_parquet = f\"read_parquet('{s3dir_t_poc.uri}*/*.parquet', hive_partitioning=1)\"\n",
    "\n",
    "# this is only one file\n",
    "s3path = s3dir_t_poc.joinpath(\"date=2021-02-01/data.parquet\")\n",
    "sql_from_table_one_parquet = f\"read_parquet('{s3path.uri}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4bcf8-30a3-48de-9e09-7f360365cbb7",
   "metadata": {},
   "source": [
    "## Example 1 - Filter by Partition and Integer Column\n",
    "\n",
    "This query leverages the Hive partition and parquet row group metadata (number of record for each row group) to avoid data scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "608b111c-5a73-4a3a-b74a-1098d77b58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┐\n",
      "│  count  │\n",
      "│  int64  │\n",
      "├─────────┤\n",
      "│ 7000000 │\n",
      "└─────────┘\n",
      "\n",
      "from 2023-09-14 02:36:16.344082 to 2023-09-14 02:36:18.235196 elapsed 1.891114 second.\n"
     ]
    }
   ],
   "source": [
    "sql = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "    COUNT(t.id) as count\n",
    "FROM {sql_from_table_all_parquet} t\n",
    "WHERE \n",
    "    (t.date BETWEEN '2021-02-01' AND '2021-02-07')\n",
    ";\n",
    "\"\"\"\n",
    ")\n",
    "with DateTimeTimer():\n",
    "    duckdb.sql(sql_httpfs)\n",
    "    duckdb.sql(sql_credential)\n",
    "    duckdb.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc2cfded-0a79-4bce-8838-361a95dc7d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 7000000\n",
      "from 2023-09-14 02:36:38.875767 to 2023-09-14 02:37:09.511694 elapsed 30.635927 second.\n"
     ]
    }
   ],
   "source": [
    "# this is how people usually do this\n",
    "with DateTimeTimer():\n",
    "    count = 0\n",
    "    for s3path in s3dir_t_poc.iter_objects():\n",
    "        if \"date=2021-02-01\" <= s3path.dirname <= \"date=2021-02-07\":\n",
    "            with s3path.open(\"rb\") as f:\n",
    "                df = pl.read_parquet(f)\n",
    "                count += df.shape[0]\n",
    "    print(f\"count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a44357-cbbc-4a92-b757-a446f040f9f9",
   "metadata": {},
   "source": [
    "## Example 2 - Filter by Partition and Integer Column\n",
    "\n",
    "This query leverages the Hive partition and parquet row group statistic metadata (min and max value of the number column in the row group) to reduce data scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bfd6d23-d657-4828-87c7-36503e0b72a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────┐\n",
      "│ count  │\n",
      "│ int64  │\n",
      "├────────┤\n",
      "│ 701051 │\n",
      "└────────┘\n",
      "\n",
      "from 2023-09-14 02:39:15.366255 to 2023-09-14 02:39:17.383965 elapsed 2.017710 second.\n"
     ]
    }
   ],
   "source": [
    "sql = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "    count(t.id) as count\n",
    "FROM {sql_from_table_all_parquet} t\n",
    "WHERE \n",
    "    (t.date BETWEEN '2021-02-01' AND '2021-02-07')\n",
    "    AND t.number BETWEEN 600000 AND 700000\n",
    ";\n",
    "\"\"\"\n",
    ")\n",
    "with DateTimeTimer():\n",
    "    duckdb.sql(sql_httpfs)\n",
    "    duckdb.sql(sql_credential)\n",
    "    duckdb.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2fc495-2ee7-4605-adca-7fbbe9a0a7fb",
   "metadata": {},
   "source": [
    "## Example 3 - Filter by Partition and Sortable String Column\n",
    "\n",
    "Just another example of range query on string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7dc1062-16e0-4083-9f97-636e740be296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────┐\n",
      "│ count  │\n",
      "│ int64  │\n",
      "├────────┤\n",
      "│ 575337 │\n",
      "└────────┘\n",
      "\n",
      "from 2023-09-14 02:39:18.727477 to 2023-09-14 02:39:22.165245 elapsed 3.437768 second.\n"
     ]
    }
   ],
   "source": [
    "sql = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "    count(t.id) as count\n",
    "FROM {sql_from_table_all_parquet} t\n",
    "WHERE \n",
    "    (t.date BETWEEN '2021-02-01' AND '2021-02-07')\n",
    "    AND (t.time_str BETWEEN '2022-06-01' AND '2022-07-01')\n",
    ";\n",
    "\"\"\"\n",
    ")\n",
    "with DateTimeTimer():\n",
    "    duckdb.sql(sql_httpfs)\n",
    "    duckdb.sql(sql_credential)\n",
    "    duckdb.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff83a8-80c4-4725-87e7-d3c686ab754f",
   "metadata": {},
   "source": [
    "## Example 4 - Full Text Search\n",
    "\n",
    "You can use local file (similar to sqlite, Duckdb is a single file embedded database) to store the data and full text search index, then perform full text search query efficiently later on. With full text search, it is blazingly fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063cf9ee-4d38-40aa-9cca-ffa47f07eba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data and create index: from 2023-09-14 02:42:40.636271 to 2023-09-14 02:42:48.828329 elapsed 8.192058 second.\n"
     ]
    }
   ],
   "source": [
    "conn = duckdb.connect(database='duckdb_poc.duckdb', read_only=False)\n",
    "\n",
    "# select 7 days data as the data source\n",
    "sql_create_table = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE corpus AS\n",
    "        SELECT\n",
    "            t.id,\n",
    "            t.number,\n",
    "            t.text\n",
    "        FROM {sql_from_table_all_parquet} t\n",
    "        WHERE \n",
    "            (t.date BETWEEN '2021-02-01' AND '2021-02-07')\n",
    "            AND t.number <= 100000\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sql_create_ftx_index = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "PRAGMA create_fts_index('corpus', 'id', 'text');\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with DateTimeTimer(title=\"load data and create index\"):\n",
    "    conn.sql(sql_httpfs)\n",
    "    conn.sql(sql_fts)\n",
    "    conn.sql(sql_credential)\n",
    "    conn.sql(sql_create_table)\n",
    "    conn.sql(sql_create_ftx_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bea4815-29b2-4a6c-bf22-94429bc0f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────┬──────────┬────────┬─────────────────────────────────────────────────────┐\n",
      "│       score       │    id    │ number │                        text                         │\n",
      "│      double       │  int64   │ int64  │                       varchar                       │\n",
      "├───────────────────┼──────────┼────────┼─────────────────────────────────────────────────────┤\n",
      "│ 4.177975388014909 │ 34212088 │  66025 │ Everybody personal political network last oil.      │\n",
      "│ 4.177975388014909 │ 37547324 │  93401 │ Concern politics network heart.                     │\n",
      "│ 4.177975388014909 │ 37698101 │  99526 │ Politics anyone style network computer.             │\n",
      "│ 4.177975388014909 │ 31212088 │  66025 │ Everybody personal political network last oil.      │\n",
      "│ 4.177975388014909 │ 32212088 │  66025 │ Everybody personal political network last oil.      │\n",
      "│ 4.177975388014909 │ 33212088 │  66025 │ Everybody personal political network last oil.      │\n",
      "│ 4.177975388014909 │ 35212088 │  66025 │ Everybody personal political network last oil.      │\n",
      "│ 4.177975388014909 │ 36547324 │  93401 │ Concern politics network heart.                     │\n",
      "│ 4.177975388014909 │ 36698101 │  99526 │ Politics anyone style network computer.             │\n",
      "│ 3.792609315788894 │ 34054471 │  77217 │ Newspaper political network international shoulder. │\n",
      "│         ·         │     ·    │    ·   │               ·                                     │\n",
      "│         ·         │     ·    │    ·   │               ·                                     │\n",
      "│         ·         │     ·    │    ·   │               ·                                     │\n",
      "│ 2.805021425786722 │ 33995213 │  98633 │ Network also institution this.                      │\n",
      "│ 2.805021425786722 │ 34011537 │  30660 │ Network unit serious.                               │\n",
      "│ 2.805021425786722 │ 34041951 │  49602 │ Night indicate especially use network.              │\n",
      "│ 2.805021425786722 │ 34080149 │  38955 │ Network south become away.                          │\n",
      "│ 2.805021425786722 │ 34143747 │  83895 │ Network customer few.                               │\n",
      "│ 2.805021425786722 │ 34153442 │  91319 │ Network choice herself together.                    │\n",
      "│ 2.805021425786722 │ 34235542 │   4305 │ Network well little fall alone for.                 │\n",
      "│ 2.805021425786722 │ 34245078 │  55023 │ Staff network possible.                             │\n",
      "│ 2.805021425786722 │ 34249988 │  73174 │ Particular network kind right.                      │\n",
      "│ 2.805021425786722 │ 34279067 │  28448 │ Often different network season do.                  │\n",
      "├───────────────────┴──────────┴────────┴─────────────────────────────────────────────────────┤\n",
      "│ 100 rows (20 shown)                                                               4 columns │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "full text search query - using index: from 2023-09-14 02:46:36.069247 to 2023-09-14 02:46:36.139054 elapsed 0.069807 second.\n"
     ]
    }
   ],
   "source": [
    "sql = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "    fts_main_corpus.match_bm25(t.id, 'political network') AS score,\n",
    "    t.id,\n",
    "    t.number,\n",
    "    t.text\n",
    "FROM corpus AS t\n",
    "WHERE score IS NOT NULL\n",
    "ORDER BY score DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with DateTimeTimer(\"full text search query - using index\"):\n",
    "    conn.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ac4f93-ea71-456c-88cb-b6ab15d23867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────┬────────┬─────────────────────────────────────────────────────────┐\n",
      "│    id    │ number │                          text                           │\n",
      "│  int64   │ int64  │                         varchar                         │\n",
      "├──────────┼────────┼─────────────────────────────────────────────────────────┤\n",
      "│ 31054471 │  77217 │ Newspaper political network international shoulder.     │\n",
      "│ 31212088 │  66025 │ Everybody personal political network last oil.          │\n",
      "│ 31272342 │   5479 │ Pm for issue network political man easy.                │\n",
      "│ 32054471 │  77217 │ Newspaper political network international shoulder.     │\n",
      "│ 32212088 │  66025 │ Everybody personal political network last oil.          │\n",
      "│ 32272342 │   5479 │ Pm for issue network political man easy.                │\n",
      "│ 33054471 │  77217 │ Newspaper political network international shoulder.     │\n",
      "│ 33212088 │  66025 │ Everybody personal political network last oil.          │\n",
      "│ 33272342 │   5479 │ Pm for issue network political man easy.                │\n",
      "│ 34054471 │  77217 │ Newspaper political network international shoulder.     │\n",
      "│ 34212088 │  66025 │ Everybody personal political network last oil.          │\n",
      "│ 34272342 │   5479 │ Pm for issue network political man easy.                │\n",
      "│ 35054471 │  77217 │ Newspaper political network international shoulder.     │\n",
      "│ 35212088 │  66025 │ Everybody personal political network last oil.          │\n",
      "│ 35272342 │   5479 │ Pm for issue network political man easy.                │\n",
      "│ 36174813 │  63406 │ Reflect easy five political budget ground book network. │\n",
      "│ 36953078 │  85116 │ Whom network prevent political inside follow sport.     │\n",
      "│ 36976413 │  81053 │ Plan day effect heavy political theory grow network.    │\n",
      "│ 37174813 │  63406 │ Reflect easy five political budget ground book network. │\n",
      "│ 37953078 │  85116 │ Whom network prevent political inside follow sport.     │\n",
      "│ 37976413 │  81053 │ Plan day effect heavy political theory grow network.    │\n",
      "├──────────┴────────┴─────────────────────────────────────────────────────────┤\n",
      "│ 21 rows                                                           3 columns │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "full text search query - without index: from 2023-09-14 02:45:08.611037 to 2023-09-14 02:45:08.635814 elapsed 0.024777 second.\n"
     ]
    }
   ],
   "source": [
    "sql = textwrap.dedent(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "    t.id,\n",
    "    t.number,\n",
    "    t.text\n",
    "FROM corpus AS t\n",
    "WHERE \n",
    "    t.text LIKE '%political%'\n",
    "    AND t.text LIKE '%network%'\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with DateTimeTimer(\"full text search query - without index\"):\n",
    "    conn.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c29315-74b8-4f50-b2bf-5736d27fdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ba305-a3d4-47a5-a786-c39eeb107488",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If the necessary data involved in the computation can be narrow down using partition, row group statistics, and it fit the memory, DuckDB is as fast as Athena (maybe event faster because you don't need to read result from S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde8f40-b0d6-4be8-9a12-cde617986ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
