{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658ef93e",
   "metadata": {},
   "source": [
    "# Parquet Baisc\n",
    "\n",
    "[CN]\n",
    "\n",
    "在大数据领域, 列式存储格式是用来存储大量数据并提供高性能查询的行业标准. 其中有两种数据格式非常流行 [Apache ORC](https://orc.apache.org/) [Apache Parquet](https://parquet.apache.org/). 其中 Parquet 要更流行一些. [Apache Arrow](https://arrow.apache.org/docs/index.html) 则是一个 in-memory analytics 的数据分析平台, 能把对这些流行的数据格式的 IO, transform 等操作整合起来的一个项目.\n",
    "\n",
    "在 Python 社区主流的用于数据分析的库是 [Pandas](https://pandas.pydata.org/). **PyArrow** 则是用 Python 操作 Apache Arrow 的一套 API, 同时可以用这套 API 操作 Parquet / ORC 数据格式. 并且提供了一套和 pandas 交互的接口.\n",
    "\n",
    "在小数据领域 pandas 基本已经够用了, 而在大数据领域, 学习 pyarrow 则是非常有必要的.\n",
    "\n",
    "---\n",
    "\n",
    "[EN]\n",
    "\n",
    "\n",
    "Ref:\n",
    "\n",
    "- https://arrow.apache.org/docs/python/parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a2290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (12.0.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rich) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rich) (4.0.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rich) (2.8.0)\n",
      "Requirement already satisfied: dataclasses<0.9,>=0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rich) (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: s3pathlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.5)\n",
      "Requirement already satisfied: pathlib-mate>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3pathlib) (1.0.2)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib-mate>=1.0.1->s3pathlib) (1.4.0)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib-mate>=1.0.1->s3pathlib) (1.5.5)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib-mate>=1.0.1->s3pathlib) (1.15.0)\n",
      "Requirement already satisfied: pycodestyle>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autopep8->pathlib-mate>=1.0.1->s3pathlib) (2.6.0)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autopep8->pathlib-mate>=1.0.1->s3pathlib) (0.10.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pathlib_mate in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib_mate) (1.15.0)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib_mate) (1.5.5)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib_mate) (1.4.0)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autopep8->pathlib_mate) (0.10.2)\n",
      "Requirement already satisfied: pycodestyle>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autopep8->pathlib_mate) (2.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyarrow) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mpire in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mpire) (0.8)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mpire) (4.62.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rich\n",
    "%pip install smart_open\n",
    "%pip install s3pathlib\n",
    "%pip install pathlib_mate\n",
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install mpire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8b10ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU = 16\n"
     ]
    }
   ],
   "source": [
    "# inspect how many CPU on this machine\n",
    "import os\n",
    "\n",
    "print(f\"Number of CPU = {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5e7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from rich import print as rprint\n",
    "from s3pathlib import S3Path\n",
    "from pathlib_mate import Path\n",
    "\n",
    "dir_here = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89fd38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slamczyj'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnd_str(length=8):\n",
    "    \"\"\"\n",
    "    Generate random length string.\n",
    "    \"\"\"\n",
    "    return \"\".join([random.choice(string.ascii_lowercase) for _ in range(length)])\n",
    "\n",
    "rnd_str(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb83aef",
   "metadata": {},
   "source": [
    "## ``pyarrow.Table`` vs ``pandas.DataFrame``\n",
    "\n",
    "在 PyArrow 中核心的 API 是 ``pyarrow.Table``. 类似于 ``pandas.DataFrame``, 是一个二维数据表结构. 但是 ``pyarrow.Table`` 的 API 没有 ``pandas.DataFrame`` 灵活. 实际操作中经常会用 ``pandas.DataFrame`` 操作数据, 然后在写入文件前转化为 ``pyarrow.Table``.\n",
    "\n",
    "其中两个核心的 API 是:\n",
    "\n",
    "- ``pyarrow.Table.from_pandas(df)``\n",
    "- ``pyarrow.Table.to_pandas()``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4245e57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rctczyjd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gzdelpep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pqagwgsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  rctczyjd\n",
       "1   2  gzdelpep\n",
       "2   3  pqagwgsx"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个 Pandas DataFarme\n",
    "n_rows = 1000\n",
    "df = pd.DataFrame()\n",
    "df[\"id\"] = range(1, n_rows+1)\n",
    "df[\"name\"] = [rnd_str(8) for _ in range(n_rows)]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae35e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 pandas.DataFrame 转化为 pyarrow.Table\n",
    "# Ref: https://arrow.apache.org/docs/python/pandas.html\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa92c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- method 1 ---\n",
      "id: int64\n",
      "name: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 470\n",
      "--- method 2 ---\n",
      "id: int64\n",
      "name: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 470\n"
     ]
    }
   ],
   "source": [
    "# 查看 Data Schema\n",
    "# method1. use Table.schema\n",
    "print(\"--- method 1 ---\")\n",
    "print(table.schema)\n",
    "\n",
    "# method2. read schema from pandas DataFrame\n",
    "print(\"--- method 2 ---\")\n",
    "schema = pa.Schema.from_pandas(df)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3071d7",
   "metadata": {},
   "source": [
    "## Parquet File 读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c11d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 pyarrow.Table 写入磁盘\n",
    "# Ref: https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html\n",
    "pq.write_table(table, Path(dir_here, \"users.parquet\").abspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da044fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rctczyjd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gzdelpep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pqagwgsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  rctczyjd\n",
       "1   2  gzdelpep\n",
       "2   3  pqagwgsx"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从 磁盘读取 pyarrow.Table 数据\n",
    "# Ref: https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html\n",
    "table = pq.read_table(Path(dir_here, \"users.parquet\").abspath)\n",
    "df = table.to_pandas()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02af0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 pyarrow.Table 写入磁盘, 不过用的是 file object API\n",
    "# Ref: https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html\n",
    "p = Path(dir_here, \"users.parquet\")\n",
    "with p.open(\"wb\") as f:\n",
    "    pq.write_table(table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb426b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rctczyjd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gzdelpep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pqagwgsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  rctczyjd\n",
       "1   2  gzdelpep\n",
       "2   3  pqagwgsx"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从 磁盘读取 pyarrow.Table 数据, 不过用的是 file object API\n",
    "# Ref: https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html\n",
    "with p.open(\"rb\") as f:\n",
    "    table = pq.read_table(f)\n",
    "df = table.to_pandas()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef7e4127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rctczyjd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gzdelpep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pqagwgsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1  rctczyjd\n",
       "1   2  gzdelpep\n",
       "2   3  pqagwgsx"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 pyarrow.parquet.ParquetFile API 读取数据, 这个 API 能提供更精细的 metadata access. \n",
    "# 但是这个 API 无法用于 write Data\n",
    "p_file = pq.ParquetFile(p.abspath)\n",
    "p_file.read().to_pandas().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad05abb",
   "metadata": {},
   "source": [
    "## Dictionary Encoding 字典编码\n",
    "\n",
    "``字典编码`` 就是将所有可能的值的总数不多的列用数字编码. 比如不超过 256 个可能值就可以用一个 8 位整数对其编码, 原始值很可能是字符串, 这样能大大节约存储空间.\n",
    "\n",
    "``use_dictionary`` 参数能控制是否自动使用字典编码, 还是只对指定的列使用. ``pyarrow`` 默认使用 dict encode, 如果不使用, 最后的文件会大一些. 特别注意的是, 被 dict encode 后的列只能用于 等于 和 不等于 的比较, 而无法利用该列的最大最小信息进行 range query 的优化.\n",
    "\n",
    "Ref:\n",
    "\n",
    "- [pyarrow.parquet.write_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d10b789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events1.parquet file size = 11.31 MB\n",
      "events2.parquet file size = 13.63 MB\n"
     ]
    }
   ],
   "source": [
    "p = Path(dir_here, \"events1.parquet\")\n",
    "with p.open(\"wb\") as f:\n",
    "    pq.write_table(\n",
    "        pa.Table.from_pandas(df), \n",
    "        f,\n",
    "        use_dictionary=True, # default is True \n",
    "    )\n",
    "print(f\"{p.basename} file size = {p.size_in_text}\")\n",
    "\n",
    "p = Path(dir_here, \"events2.parquet\")\n",
    "with p.open(\"wb\") as f:\n",
    "    pq.write_table(\n",
    "        pa.Table.from_pandas(df), \n",
    "        f,\n",
    "        use_dictionary=False,\n",
    "    )\n",
    "print(f\"{p.basename} file size = {p.size_in_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c38f23",
   "metadata": {},
   "source": [
    "### Row Group 行组\n",
    "\n",
    "Row Group 是把一个 Table 分拆成很多小的 \"Table\", 这些小的 \"Table\" 就是 Row Group. 每个 Row Group 有 metadata 和 statistics, 记录了每个 Column 的 max / min. 这样扫描数据的时候可以利用这些统计信息, 从而跳过很多 Row Group. 但是这样做的代价就是如果 Row Group 太多, metadata 也会太多, 写入速度也会变慢, 读取整块数据的速度也会变慢."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9344a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 1000000 行\n",
      "一共有 1000 个 row group, events1.parquet file size = 14.75 MB\n",
      "一共有 1 个 row group, events2.parquet file size = 11.31 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"一共有 {df.shape[0]} 行\")\n",
    "# row group 越多, 占用磁盘越多, 但是扫描起来越有可能跳过不必要的数据块. row group 的数量最终需要取比较平衡的值.\n",
    "p = Path(dir_here, \"events1.parquet\")\n",
    "row_group_size = 1000\n",
    "n_row_groups = int(df.shape[0] / row_group_size)\n",
    "with p.open(\"wb\") as f:\n",
    "    pq.write_table(\n",
    "        pa.Table.from_pandas(df), \n",
    "        f,\n",
    "        row_group_size=row_group_size,\n",
    "    )\n",
    "print(f\"一共有 {n_row_groups} 个 row group, {p.basename} file size = {p.size_in_text}\")\n",
    "\n",
    "p = Path(dir_here, \"events2.parquet\")\n",
    "row_group_size = 1000000\n",
    "n_row_groups = int(df.shape[0] / row_group_size)\n",
    "with p.open(\"wb\") as f:\n",
    "    pq.write_table(\n",
    "        pa.Table.from_pandas(df), \n",
    "        f,\n",
    "        row_group_size=row_group_size,\n",
    "    )\n",
    "print(f\"一共有 {n_row_groups} 个 row group, {p.basename} file size = {p.size_in_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6625b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2a65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.Statistics object at 0x7f7b69bced18>\n",
      "  has_min_max: True\n",
      "  min: 2000-01-01 00:00:00\n",
      "  max: 2000-01-01 00:44:35.723000\n",
      "  null_count: 0\n",
      "  distinct_count: 0\n",
      "  num_values: 1000\n",
      "  physical_type: INT64\n",
      "  logical_type: Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false)\n",
      "  converted_type (legacy): NONE\n",
      "pyarrow.Table\n",
      "event_id: int64\n",
      "event_type: string\n",
      "time: timestamp[us]\n",
      "value: int64\n",
      "----\n",
      "event_id: [[1,2,3,4,5,6,7,8,9,10,...,991,992,993,994,995,996,997,998,999,1000]]\n",
      "event_type: [[\"event_type_2\",\"event_type_1\",\"event_type_3\",\"event_type_2\",\"event_type_2\",\"event_type_1\",\"event_type_3\",\"event_type_3\",\"event_type_1\",\"event_type_1\",...,\"event_type_2\",\"event_type_2\",\"event_type_3\",\"event_type_3\",\"event_type_1\",\"event_type_2\",\"event_type_1\",\"event_type_3\",\"event_type_2\",\"event_type_1\"]]\n",
      "time: [[2000-01-01 00:00:00.000000,2000-01-01 00:00:02.678000,2000-01-01 00:00:05.356000,2000-01-01 00:00:08.035000,2000-01-01 00:00:10.713000,2000-01-01 00:00:13.392000,2000-01-01 00:00:16.070000,2000-01-01 00:00:18.748000,2000-01-01 00:00:21.427000,2000-01-01 00:00:24.105000,...,2000-01-01 00:44:11.617000,2000-01-01 00:44:14.296000,2000-01-01 00:44:16.974000,2000-01-01 00:44:19.652000,2000-01-01 00:44:22.331000,2000-01-01 00:44:25.009000,2000-01-01 00:44:27.688000,2000-01-01 00:44:30.366000,2000-01-01 00:44:33.044000,2000-01-01 00:44:35.723000]]\n",
      "value: [[43,89,58,94,5,64,86,49,98,74,...,85,92,95,20,99,98,54,85,97,55]]\n",
      "<pyarrow._parquet.ColumnChunkMetaData object at 0x7f7b69bcec78>\n",
      "  file_offset: 5364\n",
      "  file_path: \n",
      "  physical_type: INT64\n",
      "  num_values: 1000\n",
      "  path_in_schema: event_id\n",
      "  is_stats_set: True\n",
      "  statistics:\n",
      "    <pyarrow._parquet.Statistics object at 0x7f7b69bce958>\n",
      "      has_min_max: True\n",
      "      min: 1\n",
      "      max: 1000\n",
      "      null_count: 0\n",
      "      distinct_count: 0\n",
      "      num_values: 1000\n",
      "      physical_type: INT64\n",
      "      logical_type: None\n",
      "      converted_type (legacy): NONE\n",
      "  compression: SNAPPY\n",
      "  encodings: ('PLAIN_DICTIONARY', 'PLAIN', 'RLE')\n",
      "  has_dictionary_page: True\n",
      "  dictionary_page_offset: 4\n",
      "  data_page_offset: 4035\n",
      "  total_compressed_size: 5360\n",
      "  total_uncompressed_size: 9341\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "p = Path(dir_here, \"events.parquet\")\n",
    "with p.open(\"wb\") as f:\n",
    "    pq.write_table(\n",
    "        pa.Table.from_pandas(df), \n",
    "        f,\n",
    "        row_group_size=1000,\n",
    "    )\n",
    "\n",
    "# columns: [\"event_id\", \"event_type\", \"time\", \"value\"]\n",
    "pq_file = pq.ParquetFile(p.abspath)\n",
    "for ith_row_group in range(pq_file.metadata.num_row_groups):\n",
    "    rg_meta = pq_file.metadata.row_group(ith_row_group)\n",
    "    print(rg_meta.column(2).statistics)\n",
    "    rg = pq_file.read_row_group(ith_row_group)\n",
    "    print(rg)\n",
    "    print(rg_meta.column(0))\n",
    "    print(rg_meta.column(0).statistics.min)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835da823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
