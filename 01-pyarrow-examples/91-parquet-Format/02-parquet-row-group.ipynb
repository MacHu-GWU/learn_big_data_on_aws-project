{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17460210",
   "metadata": {},
   "source": [
    "# Parquet Row Group\n",
    "\n",
    "首先我们来了解下 Parquet 中的一些重要概念 (https://github.com/apache/parquet-format):\n",
    "\n",
    "[EN]\n",
    "\n",
    "- Block (HDFS block): This means a block in HDFS and the meaning is unchanged for describing this file format. The file format is designed to work well on top of HDFS.\n",
    "- File: A HDFS file that must include the metadata for the file. It does not need to actually contain the data.\n",
    "- Row group: A logical horizontal partitioning of the data into rows. There is no physical structure that is guaranteed for a row group. A row group consists of a column chunk for each column in the dataset.\n",
    "- Column chunk: A chunk of the data for a particular column. They live in a particular row group and are guaranteed to be contiguous in the file.\n",
    "- Page: Column chunks are divided up into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). There can be multiple page types which are interleaved in a column chunk.\n",
    "\n",
    "[CN]\n",
    "\n",
    "考虑一个 DataFrame events, 有 event_id: string(32) 和 event_time: datetime64 两列. 一共 1,000,000 (1M) 行.\n",
    "\n",
    "- Block: HDFS 分布式文件系统中的数据块, 一般是 128 MB 一个. 也有用 64 MB 或是 256 MB 的.\n",
    "- File: 一个具体的文件. 里面包含了 metadata.\n",
    "- Row group: 一个文件内部从逻辑上被分为了很多个 Row Group, 每个 Row Group 的起始位置和结束位置的指针是保存在 File metadata 中的. 一个 Row Group 会包含很多 \"行\" (虽然数据不是按行存储的) 的数据. 比如我们可以说一个 Parquet 文件有 100 个 Row Group, 每个 Row Group 有 10,000 行数据.\n",
    "- Column chunk: 一个 Row Group 中会有很多 Column. Column chunk 内的数据是连续存储的. 比如从 0 ~ 1,000,000,000 bytes 是 event_id, 1,000,000,001 ~ 1,005,000,000 是 event_time.\n",
    "\n",
    "Row Group 对用列式存储进行分析非常有帮助. 我们考虑一个例子: 我们需要读取一个大文件, 其中有个 column 是叫 event_time, 我们只需要 event_time between (start, end) 的数据. 有了 Row Group, 你可以通过 File metadata 直接定位到每个 Row Group 的起始点, 并读取 Row Group 的 metadata, 里面就有这个 Row Group 的 event_time 最大最小值. 这样可以避免扫描许多 Row Group, 从而提高查询速度.\n",
    "\n",
    "Ref:\n",
    "\n",
    "- https://arrow.apache.org/docs/python/parquet.html#finer-grained-reading-and-writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d15d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib_mate in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib_mate) (1.15.0)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib_mate) (1.4.0)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathlib_mate) (1.5.5)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autopep8->pathlib_mate) (0.10.2)\n",
      "Requirement already satisfied: pycodestyle>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autopep8->pathlib_mate) (2.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sfm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.0.17)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sfm) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pathlib_mate\n",
    "%pip install sfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2b923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from datetime import datetime\n",
    "from pathlib_mate import Path\n",
    "from sfm.timer import DateTimeTimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7026ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 2022-03-16 19:35:40.603381 to 2022-03-16 19:35:40.625431 elapsed 0.022050 second.\n"
     ]
    }
   ],
   "source": [
    "n_rows = 1_000_000\n",
    "\n",
    "def gen_data(n_rows):\n",
    "    event_type_list = [\"event_type_1\", \"event_type_2\", \"event_type_3\"]\n",
    "    data_table = pa.table({\n",
    "        \"event_id\": np.arange(n_rows),\n",
    "        \"event_type\": pa.DictionaryArray.from_arrays(\n",
    "            np.random.randint(low=0, high=len(event_type_list), size=n_rows), \n",
    "            event_type_list,\n",
    "        ),\n",
    "        \"event_time\": pd.date_range(\"2000-01-01\", periods=n_rows, freq=\"1s\")\n",
    "    })\n",
    "    return data_table\n",
    "\n",
    "with DateTimeTimer():\n",
    "    data_table = gen_data(n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600c1bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>event_type_3</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>event_type_3</td>\n",
       "      <td>2000-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>event_type_1</td>\n",
       "      <td>2000-01-01 00:00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id    event_type          event_time\n",
       "0         0  event_type_3 2000-01-01 00:00:00\n",
       "1         1  event_type_3 2000-01-01 00:00:01\n",
       "2         2  event_type_1 2000-01-01 00:00:02"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.take([0, 1, 2]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd8939a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999997</td>\n",
       "      <td>event_type_3</td>\n",
       "      <td>2000-01-12 13:46:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999998</td>\n",
       "      <td>event_type_2</td>\n",
       "      <td>2000-01-12 13:46:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999999</td>\n",
       "      <td>event_type_3</td>\n",
       "      <td>2000-01-12 13:46:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id    event_type          event_time\n",
       "0    999997  event_type_3 2000-01-12 13:46:37\n",
       "1    999998  event_type_2 2000-01-12 13:46:38\n",
       "2    999999  event_type_3 2000-01-12 13:46:39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.take([n_rows - 3, n_rows - 2, n_rows - 1]).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e880d2a",
   "metadata": {},
   "source": [
    "## Optimize Parquet file Using Row Group\n",
    "\n",
    "用 Row Group 对 查询进行优化的原理是, 如果你有 1GB 的数据, 而你的查询结果可能只涉及到 10MB 的数据, 那么 Row Group 对于每个 Column 的统计信息就可以帮助跳过哪些不包含目标数据的 Row Group. 很有可能你只需要扫描不到 100MB 数据就可以得到结果了. 而 IO 相比基于内存向量的查询要慢几个数量级, 所以最后扫描的速度相比把整个数据集读到内存要快得多的多.\n",
    "\n",
    "用 Row Group 进行优化, 建议按照这个流程:\n",
    "\n",
    "1. 决定哪几个 Column 你会进行 Range Query, 通常是 timestamp 列. 选择最重要的 Range Query Column.\n",
    "2. 在写入磁盘之前, 对 Range Query Column 进行排序.\n",
    "\n",
    "知识点:\n",
    "\n",
    "- ``row_group_size`` 的默认值为 64 * 1024 * 1024 = 64M\n",
    "- ``pyarrow.parquet.ParquetFile.num_row_groups`` 属性能获得 parquet 文件的 row group 总数\n",
    "- ``pyarrow.parquet.ParquetFile.metadata.row_group(row_group_id: int)`` 方法能获得具体某个 row group 的 metadata\n",
    "- ``pyarrow.parquet.ParquetFile.metadata.row_group(row_group_id: int).column(column_id: int)`` 方法能获得具体某个 row group 中某个 column 的 metadata\n",
    "- ``pyarrow.parquet.ParquetFile.metadata.row_group(row_group_id: int).column(column_id: int).statistics.min / max`` 属性能获得具体某个 row group 中某个 column 的统计最大值, 最小值.\n",
    "\n",
    "Ref:\n",
    "\n",
    "- https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac5b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file = Path(\"events_with_row_group.parquet\")\n",
    "pq.write_table(data_table, pq_file.abspath, row_group_size=int(n_rows / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cf463b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13.15 MB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_file.size_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdde7e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 2022-03-16 19:35:40.954322 to 2022-03-16 19:35:40.989486 elapsed 0.035164 second.\n"
     ]
    }
   ],
   "source": [
    "start = datetime(2000, 1, 5)\n",
    "end = datetime(2000, 1, 6)\n",
    "\n",
    "with DateTimeTimer():\n",
    "    parquet_file = pq.ParquetFile(pq_file.abspath)\n",
    "\n",
    "    tables = list()\n",
    "    for rg_id in range(parquet_file.num_row_groups):\n",
    "        rg_meta = parquet_file.metadata.row_group(rg_id)\n",
    "        event_time_col = rg_meta.column(2)\n",
    "        if not ((event_time_col.statistics.max < start) or (event_time_col.statistics.min > end)):\n",
    "            t = parquet_file.read_row_group(rg_id)\n",
    "            t_filtered = pc.filter(\n",
    "                t,\n",
    "                pc.and_(\n",
    "                    pc.greater_equal(t[\"event_time\"], start),\n",
    "                    pc.less(t[\"event_time\"], end),\n",
    "                ),\n",
    "            )\n",
    "            tables.append(t_filtered)\n",
    "\n",
    "    table_filtered = pa.concat_tables(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad78dfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86400, 3)\n",
      "min event_time is: 2000-01-05 00:00:00\n",
      "max event_time is: 2000-01-05 23:59:59\n"
     ]
    }
   ],
   "source": [
    "print(table_filtered.shape)\n",
    "print(\"min event_time is:\", pc.min(table_filtered[\"event_time\"]))\n",
    "print(\"max event_time is:\", pc.max(table_filtered[\"event_time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9d5b90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 2022-03-16 19:35:40.998017 to 2022-03-16 19:35:41.177411 elapsed 0.179394 second.\n"
     ]
    }
   ],
   "source": [
    "with DateTimeTimer():\n",
    "    t = pq.read_table(pq_file.abspath)\n",
    "    t_filtered = pc.filter(\n",
    "        t,\n",
    "        pc.and_(\n",
    "            pc.greater_equal(t[\"event_time\"], start),\n",
    "            pc.less(t[\"event_time\"], end),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c034026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86400, 3)\n",
      "min event_time is: 2000-01-05 00:00:00\n",
      "max event_time is: 2000-01-05 23:59:59\n"
     ]
    }
   ],
   "source": [
    "print(table_filtered.shape)\n",
    "print(\"min event_time is:\", pc.min(table_filtered[\"event_time\"]))\n",
    "print(\"max event_time is:\", pc.max(table_filtered[\"event_time\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
